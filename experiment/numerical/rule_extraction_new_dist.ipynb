{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold, datasets\n",
    "import pickle\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.covariance import MinCovDet\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math\n",
    "\n",
    "class DetectorEnsemble:\n",
    "    def __init__(self):\n",
    "        self.detectors = []\n",
    "        '''\n",
    "        self.detectors.append(('iforest1', IsolationForest(random_state = 0, max_samples = 128, n_estimators = 100)))\n",
    "        self.detectors.append(('iforest2', IsolationForest(random_state = 0, max_samples = 128, n_estimators = 200)))\n",
    "        self.detectors.append(('iforest3', IsolationForest(random_state = 0, max_samples = 256, n_estimators = 100)))\n",
    "        self.detectors.append(('iforest4', IsolationForest(random_state = 0, max_samples = 256, n_estimators = 200)))\n",
    "        self.detectors.append(('iforest5', IsolationForest(random_state = 0, max_samples = 512, n_estimators = 100)))\n",
    "        self.detectors.append(('iforest6', IsolationForest(random_state = 0, max_samples = 512, n_estimators = 200)))\n",
    "        '''\n",
    "        self.detectors.append(('knn', NearestNeighbors(algorithm='ball_tree')))\n",
    "        self.detectors.append(('lof', LocalOutlierFactor(metric=\"precomputed\")))\n",
    "        #self.detectors.append(('robustcov', MinCovDet()))\n",
    "        self.detectors.append(('iforest', IsolationForest()))\n",
    "        self.detectors.append(('ocsvm', OneClassSVM()))\n",
    "        self.detectors.append(('dbscan',  DBSCAN()))\n",
    "    \n",
    "    def fit_detector(self, X, y):\n",
    "        self.clf = LinearRegression(fit_intercept=True, normalize=False, copy_X=True).fit(X, y)\n",
    "\n",
    "    def fit(self, mat):\n",
    "        dist = pairwise_distances(X = mat, metric='euclidean')\n",
    "        self.scores = []\n",
    "        for (name, detector) in self.detectors:\n",
    "            if name[:3] == 'lof':\n",
    "                detector.fit_predict(dist)\n",
    "                self.scores.append(-detector.negative_outlier_factor_)\n",
    "            elif name == 'robustcov':\n",
    "                detector.fit(mat)\n",
    "                self.scores.append(detector.mahalanobis(mat))\n",
    "            elif name == 'knn':\n",
    "                detector.fit(mat)\n",
    "                self.scores.append(-detector.kneighbors(mat)[0][:, -1])\n",
    "            elif name == 'dbscan':\n",
    "                detector.fit(mat)\n",
    "                score = np.array([1 if x == -1 else 0 for x in detector.labels_])\n",
    "                self.scores.append(score)\n",
    "            else:\n",
    "                detector.fit_predict(mat)\n",
    "                self.scores.append(-detector.score_samples(mat))\n",
    "            print(name, min(self.scores[-1]), max(self.scores[-1]), self.scores[-1].shape)\n",
    "        tmp = []\n",
    "        for score in self.scores:\n",
    "            min_s = np.min(score)\n",
    "            max_s = np.max(score)\n",
    "            range_s = max(1, max_s - min_s)\n",
    "            score = (score - min_s) / range_s\n",
    "            tmp.append(score)\n",
    "        self.n = mat.shape[0]\n",
    "        self.scores = np.array(tmp)\n",
    "        self.ground_truth = {}\n",
    "        self.adjust_sample_weight = self.n // 100\n",
    "        self.weights = np.ones(len(self.detectors))\n",
    "        weights = self.weights / np.sum(self.weights)\n",
    "\n",
    "        self.scores = self.scores.transpose()\n",
    "        y = (self.scores * weights).sum(axis = 1)\n",
    "        print('before fit', self.scores.shape, y.shape)\n",
    "        self.fit_detector(self.scores, y)\n",
    "        print('after fit')\n",
    "    \n",
    "    def weighted_score(self):\n",
    "        y = self.clf.predict(self.scores)\n",
    "        for i in self.ground_truth:\n",
    "            y[i] = self.ground_truth[i]\n",
    "        return y\n",
    "\n",
    "    def adjust_weight(self, idx, score):\n",
    "        self.ground_truth[idx] = score\n",
    "        sample_weight = np.ones(self.n)\n",
    "        for i in self.ground_truth:\n",
    "            sample_weight[i] = self.adjust_sample_weight\n",
    "        y = self.weighted_score()\n",
    "        self.fit_detector(self.scores, y)\n",
    "\n",
    "model = pickle.load(open('../../output/dump/german0315v2.pkl', 'rb'))\n",
    "paths = model['paths']\n",
    "features = model['features']\n",
    "mat = np.array([p['sample'] for p in paths]).astype('float')\n",
    "for i in range(mat.shape[0]):\n",
    "    mat[i] = mat[i] > 0\n",
    "    mat[i] /= mat[i].sum() #np.sqrt(mat[i].sum())\n",
    "all_dist = pairwise_distances(X = mat, metric='euclidean')\n",
    "\n",
    "expected_count = 50\n",
    "expected_one_class_count = 40\n",
    "\n",
    "output_labels = ['reject', 'accept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_encoding = {\n",
    "    'credit_risk' : ['No', 'Yes'], \n",
    "    'credit_history' : [\n",
    "        \"delay in paying off in the past\",\n",
    "        \"critical account/other credits elsewhere\",\n",
    "        \"no credits taken/all credits paid back duly\",\n",
    "        \"existing credits paid back duly till now\",\n",
    "        \"all credits at this bank paid back duly\",\n",
    "    ],\n",
    "    'purpose' : [\n",
    "        \"others\",\n",
    "        \"car (new)\",\n",
    "        \"car (used)\",\n",
    "        \"furniture/equipment\",\n",
    "        \"radio/television\",\n",
    "        \"domestic appliances\",\n",
    "        \"repairs\",\n",
    "        \"education\",\n",
    "        \"vacation\",\n",
    "        \"retraining\",\n",
    "        \"business\"\n",
    "    ],\n",
    "    'installment_rate': [\"< 20\", \"20 <= ... < 25\",  \"25 <= ... < 35\", \">= 35\"],\n",
    "    'present_residence': [\n",
    "        \"< 1 yr\", \n",
    "        \"1 <= ... < 4 yrs\",\n",
    "        \"4 <= ... < 7 yrs\", \n",
    "        \">= 7 yrs\"\n",
    "    ],\n",
    "    'number_credits': [\"1\", \"2~3\", \"4~5\", \">= 6\"],\n",
    "    'people_liable': [\"0 to 2\", \"3 or more\"],\n",
    "    'savings': [\n",
    "        \"unknown/no savings account\",\n",
    "        \"... <  100 DM\", \n",
    "        \"100 <= ... <  500 DM\",\n",
    "        \"500 <= ... < 1000 DM\", \n",
    "        \"... >= 1000 DM\",\n",
    "    ],\n",
    "    'employment_duration': [\n",
    "        \"unemployed\", \n",
    "        \"< 1 yr\", \n",
    "        \"1 <= ... < 4 yrs\",\n",
    "        \"4 <= ... < 7 yrs\", \n",
    "        \">= 7 yrs\"\n",
    "    ],\n",
    "    'personal_status_sex': [\n",
    "        \"not married male\",\n",
    "        \"married male\",\n",
    "    ],\n",
    "    'other_debtors': [\n",
    "        'none',\n",
    "        'co-applicant',\n",
    "        'guarantor'\n",
    "    ],\n",
    "    'property': [\n",
    "        \"real estate\",\n",
    "        \"building soc. savings agr./life insurance\", \n",
    "        \"car or other\",\n",
    "        \"unknown / no property\",\n",
    "    ],\n",
    "    'other_installment_plans': ['bank', 'stores', 'none'],\n",
    "    'housing': [\"rent\", \"own\", \"for free\"],\n",
    "    'job': [\n",
    "        'unemployed/ unskilled - non-resident',\n",
    "        'unskilled - resident',\n",
    "        'skilled employee / official',\n",
    "        'management/ self-employed/ highly qualified employee/ officer'\n",
    "    ],\n",
    "    'status': [\n",
    "        \"no checking account\",\n",
    "        \"... < 0 DM\",\n",
    "        \"0<= ... < 200 DM\",\n",
    "        \"... >= 200 DM / salary for at least 1 year\",\n",
    "    ],\n",
    "    'telephone': ['No', 'Yes'],\n",
    "    'foreign_worker': ['No', 'Yes'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 paths have been processed.\n",
      "100 paths have been processed.\n",
      "200 paths have been processed.\n",
      "300 paths have been processed.\n",
      "400 paths have been processed.\n",
      "500 paths have been processed.\n",
      "600 paths have been processed.\n",
      "700 paths have been processed.\n",
      "800 paths have been processed.\n",
      "900 paths have been processed.\n",
      "1000 paths have been processed.\n",
      "1100 paths have been processed.\n",
      "1200 paths have been processed.\n",
      "1300 paths have been processed.\n",
      "1400 paths have been processed.\n",
      "1500 paths have been processed.\n",
      "1600 paths have been processed.\n",
      "1700 paths have been processed.\n",
      "1800 paths have been processed.\n",
      "1900 paths have been processed.\n",
      "2000 paths have been processed.\n",
      "2100 paths have been processed.\n",
      "2200 paths have been processed.\n",
      "2300 paths have been processed.\n",
      "2400 paths have been processed.\n",
      "2500 paths have been processed.\n",
      "2600 paths have been processed.\n",
      "2700 paths have been processed.\n",
      "2800 paths have been processed.\n",
      "2900 paths have been processed.\n",
      "3000 paths have been processed.\n",
      "3100 paths have been processed.\n",
      "3200 paths have been processed.\n",
      "3300 paths have been processed.\n",
      "3400 paths have been processed.\n",
      "3500 paths have been processed.\n",
      "3600 paths have been processed.\n",
      "3700 paths have been processed.\n",
      "3800 paths have been processed.\n",
      "3900 paths have been processed.\n",
      "4000 paths have been processed.\n",
      "4100 paths have been processed.\n",
      "4200 paths have been processed.\n",
      "4300 paths have been processed.\n",
      "4400 paths have been processed.\n",
      "4500 paths have been processed.\n",
      "4600 paths have been processed.\n",
      "4700 paths have been processed.\n",
      "4800 paths have been processed.\n",
      "4900 paths have been processed.\n",
      "5000 paths have been processed.\n",
      "5100 paths have been processed.\n",
      "5200 paths have been processed.\n",
      "5300 paths have been processed.\n",
      "5400 paths have been processed.\n",
      "5500 paths have been processed.\n",
      "5600 paths have been processed.\n",
      "5700 paths have been processed.\n",
      "5800 paths have been processed.\n",
      "5900 paths have been processed.\n",
      "6000 paths have been processed.\n",
      "6100 paths have been processed.\n",
      "6200 paths have been processed.\n",
      "6300 paths have been processed.\n",
      "6400 paths have been processed.\n",
      "6500 paths have been processed.\n",
      "6600 paths have been processed.\n",
      "6700 paths have been processed.\n",
      "6800 paths have been processed.\n",
      "6900 paths have been processed.\n",
      "7000 paths have been processed.\n",
      "7100 paths have been processed.\n",
      "7200 paths have been processed.\n",
      "7300 paths have been processed.\n",
      "7400 paths have been processed.\n",
      "7500 paths have been processed.\n",
      "7600 paths have been processed.\n",
      "7700 paths have been processed.\n",
      "7800 paths have been processed.\n",
      "7900 paths have been processed.\n",
      "8000 paths have been processed.\n",
      "8100 paths have been processed.\n",
      "8200 paths have been processed.\n",
      "8300 paths have been processed.\n",
      "8400 paths have been processed.\n",
      "8500 paths have been processed.\n",
      "8600 paths have been processed.\n",
      "8700 paths have been processed.\n",
      "8800 paths have been processed.\n",
      "8900 paths have been processed.\n",
      "9000 paths have been processed.\n",
      "9100 paths have been processed.\n",
      "9200 paths have been processed.\n",
      "9300 paths have been processed.\n",
      "9400 paths have been processed.\n",
      "9500 paths have been processed.\n",
      "9600 paths have been processed.\n",
      "9700 paths have been processed.\n",
      "9800 paths have been processed.\n",
      "9900 paths have been processed.\n",
      "10000 paths have been processed.\n",
      "10100 paths have been processed.\n",
      "10200 paths have been processed.\n",
      "10300 paths have been processed.\n",
      "10400 paths have been processed.\n"
     ]
    }
   ],
   "source": [
    "feature_by_name = {}\n",
    "\n",
    "for feature in features:\n",
    "    feature_by_name[feature['name']] = feature\n",
    "\n",
    "data = pd.read_csv('../model/data/german.csv')\n",
    "for col in data.columns:\n",
    "    max_v = data[col].max()\n",
    "    min_v = data[col].min()\n",
    "    if min_v == 1:\n",
    "        data[col] -= 1\n",
    "\n",
    "feature_mat = []\n",
    "for it, p in enumerate(paths):\n",
    "    if it % 100 == 0:\n",
    "        print('%d paths have been processed.' % it)\n",
    "    embed = []\n",
    "    for index, feature in enumerate(features):\n",
    "        if index in p['range']:\n",
    "            range_ = p['range'][index]\n",
    "            val = data[feature['name']]\n",
    "            #print(range_)\n",
    "            #print(feature['name'])\n",
    "            if feature['dtype'] == 'number':\n",
    "                embed.append(np.array(((val >= range_[0]) & (val <= range_[1])).astype(int)))\n",
    "            elif len(range_) == 2 and val.max() > 1:\n",
    "                embed.append(np.array(((val >= range_[0]) & (val <= range_[1])).astype(int)))\n",
    "            else:\n",
    "                #print(len(range_), val.max())\n",
    "                embed.append(np.array([range_[i] for i in val]))\n",
    "        else:\n",
    "            embed.append(np.zeros(len(data)))\n",
    "    embed = np.concatenate(embed)\n",
    "    feature_mat.append(embed)\n",
    "feature_mat = np.array(feature_mat)\n",
    "feature_dist = pairwise_distances(X = feature_mat, metric='cosine')\n",
    "\n",
    "\n",
    "sample_mat = np.array([p['sample'] for p in paths]).astype('float')\n",
    "for i in range(sample_mat.shape[0]):\n",
    "    sample_mat[i] = sample_mat[i] > 0\n",
    "    sample_mat[i] /= np.sqrt(sample_mat[i].sum())\n",
    "sample_dist = pairwise_distances(X = sample_mat, metric='euclidean')\n",
    "all_dist = sample_dist + feature_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10423, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1000)\n",
    "new_feature_mat = pca.fit_transform(feature_mat)\n",
    "print(new_feature_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_mat shape (10423, 2000)\n",
      "knn -60.613529842767015 -1.4142135623730951 (10423,)\n",
      "lof 0.9533603164386572 1.3885540164809769 (10423,)\n",
      "iforest 0.29889968414006535 0.40539423056390256 (10423,)\n",
      "ocsvm -967.5815332998574 -258.54513306962053 (10423,)\n",
      "dbscan 1 1 (10423,)\n",
      "before fit (10423, 5) (10423,)\n",
      "after fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n"
     ]
    }
   ],
   "source": [
    "all_dist = sample_dist + feature_dist\n",
    "all_mat = np.concatenate((sample_mat,  new_feature_mat), axis = 1)\n",
    "print('all_mat shape', all_mat.shape)\n",
    "\n",
    "ensemble = DetectorEnsemble()\n",
    "ensemble.fit(all_mat)\n",
    "selected_path_idxes = ensemble.weighted_score().argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_type = [\n",
    "    1,1,1,1,1, 0,0,0,0,1, \n",
    "    0,0,0,1,1, 0,0,0,1,0,\n",
    "    0,1,0,1,1, 0,0,0,0,1,\n",
    "    1,1,0,0,0, 0,0,1,0,1,\n",
    "    0,0,1,0,1, 0,0,1,0,1\n",
    "]\n",
    "\n",
    "topk = 5\n",
    "new_idxes = []\n",
    "new_dists = []\n",
    "new_pos = []\n",
    "for it, idx in enumerate(selected_path_idxes):\n",
    "    if it >= len(rule_type):\n",
    "        break\n",
    "    if rule_type[it] > 0:\n",
    "        nearest = all_dist[idx, :].argsort()[:topk]\n",
    "        dists = all_dist[idx, nearest]\n",
    "        new_idxes += nearest.tolist()\n",
    "        new_dists += dists.tolist()\n",
    "        new_pos += [it] * topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def interpret_path(path, features):\n",
    "    conds = []\n",
    "    for key in path['range']:\n",
    "        feature = features[key]\n",
    "        values = path['range'][key]\n",
    "        name = feature['name']\n",
    "        op = 'is'\n",
    "        value = ''\n",
    "        if feature['dtype'] == 'category':\n",
    "            if len(values) < len(feature['values']):\n",
    "                t_values = [1 if (i >= values[0] and i <= values[1]) else 0 for i in range(1, len(feature['values']) + 1)]\n",
    "                values = t_values\n",
    "            is_negation = np.sum(values) + 1 == len(values)\n",
    "            if is_negation:\n",
    "                op = 'is not'\n",
    "                for i, d in enumerate(values):\n",
    "                    if d == 0:\n",
    "                        value = feature['values'][i]\n",
    "                        break\n",
    "            else:\n",
    "                for i, d in enumerate(values):\n",
    "                    if d == 1:\n",
    "                        value = value + ' or ' + feature['values'][i]\n",
    "                value = value[4:]\n",
    "        else:\n",
    "            op = 'in'\n",
    "            value = '%d ~ %d' % (values[0], values[1])\n",
    "        conds.append((name, op, value))\n",
    "    output_label = output_labels[path['output']]\n",
    "    # print(output_labels, path['output'])\n",
    "    return conds, output_label\n",
    "\n",
    "for index, feature in enumerate(features):\n",
    "    if feature['name'] in current_encoding:\n",
    "        feature['values'] = current_encoding[feature['name']]\n",
    "    else:\n",
    "        feature['values'] = feature['range']\n",
    "\n",
    "rules = []\n",
    "class_count = {}\n",
    "max_n_conds = 0\n",
    "for it, i in enumerate(new_idxes):\n",
    "    conds, output = interpret_path(paths[i], features)\n",
    "    if class_count.get(output, 0) >= expected_one_class_count:\n",
    "        continue\n",
    "    class_count[output] = class_count.get(output, 0) + 1\n",
    "    rules.append({'cond': conds, 'predict': output, 'index': i, 'dist': new_dists[it] })\n",
    "    max_n_conds = max(len(conds), max_n_conds)\n",
    "    #if len(rules) >= expected_count:\n",
    "    #    break\n",
    "conds_per_line = 4\n",
    "max_n_conds = math.ceil(max_n_conds / conds_per_line) * conds_per_line\n",
    "\n",
    "\n",
    "rule_idxes = [rule['index'] for rule in rules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967\n",
      "6854\n",
      "963\n",
      "965\n",
      "966\n",
      "966\n",
      "1858\n",
      "965\n",
      "964\n",
      "963\n",
      "965\n",
      "7758\n",
      "7757\n",
      "964\n",
      "966\n",
      "963\n",
      "1753\n",
      "964\n",
      "965\n",
      "966\n",
      "964\n",
      "10180\n",
      "966\n",
      "963\n",
      "965\n",
      "2951\n",
      "7917\n",
      "5259\n",
      "2876\n",
      "9572\n",
      "2955\n",
      "2876\n",
      "3275\n",
      "2951\n",
      "2952\n",
      "2956\n",
      "4598\n",
      "6260\n",
      "2952\n",
      "2953\n",
      "7441\n",
      "7438\n",
      "7440\n",
      "7439\n",
      "7501\n",
      "4078\n",
      "4080\n",
      "4471\n",
      "4077\n",
      "4076\n",
      "9349\n",
      "4148\n",
      "4147\n",
      "5072\n",
      "9781\n",
      "3994\n",
      "3993\n",
      "3018\n",
      "3996\n",
      "2529\n",
      "9350\n",
      "4068\n",
      "8239\n",
      "9352\n",
      "9351\n",
      "9351\n",
      "9349\n",
      "9352\n",
      "9350\n",
      "9348\n",
      "2020\n",
      "2019\n",
      "2018\n",
      "2017\n",
      "2021\n",
      "10154\n",
      "10157\n",
      "9846\n",
      "10153\n",
      "10152\n"
     ]
    }
   ],
   "source": [
    "f = open('3.csv', 'w')\n",
    "\n",
    "for it, rule in enumerate(rules):\n",
    "    print(new_idxes[it])\n",
    "    if it % topk == 0:\n",
    "        s = '' + str(new_pos[it])\n",
    "    else:\n",
    "        s = 'dist: %.4f' % (new_dists[it])\n",
    "    line = 0\n",
    "    n_conds = len(rule['cond'])\n",
    "    n_lines = math.ceil(n_conds / conds_per_line)\n",
    "    base = it - it % topk\n",
    "    overlap = np.sum(np.array(paths[rule['index']]['sample']) * np.array(paths[rules[base]['index']]['sample']))\n",
    "\n",
    "    for line in range(n_lines):\n",
    "        if line == 0:\n",
    "            s += ',%d,%d,%d,IF,' % (rule['index'], np.sum(paths[rule['index']]['sample']), overlap)\n",
    "        else:\n",
    "            s += ',,,,,'\n",
    "        for pos in range(conds_per_line):\n",
    "            i = pos + line * conds_per_line\n",
    "            if i < n_conds:\n",
    "                item = rule['cond'][i]\n",
    "                s += item[0] + ',' + item[1] + ',' + item[2] + ','\n",
    "                s += 'AND,' if i < n_conds - 1 else ','\n",
    "            else:\n",
    "                s += '...,...,...,...,'\n",
    "        if line == n_lines - 1:\n",
    "            s += 'THEN,' + rule['predict']\n",
    "        s += '\\n'\n",
    "    f.write(s + '\\n')\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_path_idxes = selected_path_idxes[:50]\n",
    "\n",
    "def interpret_path(path, features):\n",
    "    conds = []\n",
    "    for key in path['range']:\n",
    "        feature = features[key]\n",
    "        values = path['range'][key]\n",
    "        name = feature['name']\n",
    "        op = 'is'\n",
    "        value = ''\n",
    "        if feature['dtype'] == 'category':\n",
    "            if len(values) < len(feature['values']):\n",
    "                t_values = [1 if (i >= values[0] and i <= values[1]) else 0 for i in range(1, len(feature['values']) + 1)]\n",
    "                values = t_values\n",
    "            is_negation = np.sum(values) + 1 == len(values)\n",
    "            if is_negation:\n",
    "                op = 'is not'\n",
    "                for i, d in enumerate(values):\n",
    "                    if d == 0:\n",
    "                        value = feature['values'][i]\n",
    "                        break\n",
    "            else:\n",
    "                for i, d in enumerate(values):\n",
    "                    if d == 1:\n",
    "                        value = value + ' or ' + feature['values'][i]\n",
    "                value = value[4:]\n",
    "        else:\n",
    "            op = 'in'\n",
    "            value = '%d ~ %d' % (values[0], values[1])\n",
    "        conds.append((name, op, value))\n",
    "    output_label = output_labels[path['output']]\n",
    "    # print(output_labels, path['output'])\n",
    "    return conds, output_label\n",
    "\n",
    "for index, feature in enumerate(features):\n",
    "    if feature['name'] in current_encoding:\n",
    "        feature['values'] = current_encoding[feature['name']]\n",
    "    else:\n",
    "        feature['values'] = feature['range']\n",
    "\n",
    "rules = []\n",
    "class_count = {}\n",
    "max_n_conds = 0\n",
    "for it, i in enumerate(selected_path_idxes):\n",
    "    conds, output = interpret_path(paths[i], features)\n",
    "    #if class_count.get(output, 0) >= expected_one_class_count:\n",
    "    #    continue\n",
    "    class_count[output] = class_count.get(output, 0) + 1\n",
    "    rules.append({'cond': conds, 'predict': output, 'index': i })\n",
    "    max_n_conds = max(len(conds), max_n_conds)\n",
    "    #if len(rules) >= expected_count:\n",
    "    #    break\n",
    "conds_per_line = 4\n",
    "max_n_conds = math.ceil(max_n_conds / conds_per_line) * conds_per_line\n",
    "\n",
    "\n",
    "rule_idxes = [rule['index'] for rule in rules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('4.csv', 'w')\n",
    "\n",
    "for it, rule in enumerate(rules):\n",
    "    s = '' + str(it)\n",
    "    line = 0\n",
    "    n_conds = len(rule['cond'])\n",
    "    n_lines = math.ceil(n_conds / conds_per_line)\n",
    "    # base = it - it % topk\n",
    "    # overlap = np.sum(np.array(paths[rule['index']]['sample']) * np.array(paths[rules[base]['index']]['sample']))\n",
    "\n",
    "    for line in range(n_lines):\n",
    "        if line == 0:\n",
    "            s += ',%d,%d,IF,' % (rule['index'], np.sum(paths[rule['index']]['sample']))#, overlap)\n",
    "        else:\n",
    "            s += ',,,,'\n",
    "        for pos in range(conds_per_line):\n",
    "            i = pos + line * conds_per_line\n",
    "            if i < n_conds:\n",
    "                item = rule['cond'][i]\n",
    "                s += item[0] + ',' + item[1] + ',' + item[2] + ','\n",
    "                s += 'AND,' if i < n_conds - 1 else ','\n",
    "            else:\n",
    "                s += '...,...,...,...,'\n",
    "        if line == n_lines - 1:\n",
    "            s += 'THEN,' + rule['predict']\n",
    "        s += '\\n'\n",
    "    f.write(s + '\\n')\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'job',\n",
       " 'range': [0, 4],\n",
       " 'importance': 0.034366424362505296,\n",
       " 'dtype': 'category',\n",
       " 'values': ['unemployed/ unskilled - non-resident',\n",
       "  'unskilled - resident',\n",
       "  'skilled employee / official',\n",
       "  'management/ self-employed/ highly qualified employee/ officer']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unemployed/ unskilled - non-resident',\n",
       " 'unskilled - resident',\n",
       " 'skilled employee / official',\n",
       " 'management/ self-employed/ highly qualified employee/ officer']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_encoding['job']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{14: [1, 0],\n",
       " 11: [0, 1, 0],\n",
       " 16: [1, 0, 0, 0],\n",
       " 19: [0, 1, 1, 1],\n",
       " 12: [0, 0, 1, 0],\n",
       " 3: [0.5, 2],\n",
       " 2: [0, 2.5],\n",
       " 18: [1, 1, 0, 1, 1],\n",
       " 6: [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths[1000]['range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{14: [1, 0],\n",
       " 11: [0, 1, 0],\n",
       " 16: [1, 0, 0, 0],\n",
       " 19: [0, 1, 1, 1],\n",
       " 12: [0, 0, 1, 0],\n",
       " 3: [0.5, 2],\n",
       " 2: [0, 2.5],\n",
       " 18: [1, 1, 0, 1, 1],\n",
       " 6: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths[1001]['range']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e68ee7780e0be540c4e3141e92f7a462f6acd183a50724e7701ea314000c600"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e68ee7780e0be540c4e3141e92f7a462f6acd183a50724e7701ea314000c600"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
